- current idea is hardware-accelerated deep-learning models
  - I'm yet to decide on which or how


### possibly important links

- [fp8 quantization](https://www.youtube.com/watch?v=GLqsETc8aTc)
- [1bit llms](https://www.microsoft.com/en-us/research/publication/the-era-of-1-bit-llms-all-large-language-models-are-in-1-58-bits/)
- [how to quantize a model in pytorch](https://oscar-savolainen.medium.com/how-to-quantize-a-neural-network-model-in-pytorch-an-in-depth-explanation-d4a2cdf632a4)